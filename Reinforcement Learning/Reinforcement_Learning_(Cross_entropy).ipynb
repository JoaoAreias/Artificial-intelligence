{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Learning (Cross entropy)",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoaoAreias/Artificial-intelligence/blob/master/Reinforcement%20Learning/Reinforcement_Learning_(Cross_entropy).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKi7KXI2QBuv"
      },
      "source": [
        "# Aprendizado por reforço\n",
        "A aprendizagem por reforço é um método de apendizado não supervisionado que consiste em simular o ambiente e permitir que um agente interaja com o ambiente.\n",
        "\n",
        "<center>![Fonte: Wikipédia](https://upload.wikimedia.org/wikipedia/commons/1/1b/Reinforcement_learning_diagram.svg)</center>\n",
        "\n",
        "Este método busca recompensar atitudes positivas do agente no ambiente e penalizar atitudes negativas.\n",
        "\n",
        "### O agente\n",
        "O agente é o modelo para o qual esta se aplicando o algorítimo de aprendizagem sendo comumente utilizadas redes neurais para o processo. Este agente recebe as variáveis de estado que descrevem o ambiente e retornam a ação pela qual ele irá interagir com o ambiente.\n",
        "\n",
        "Uma vez a ação é executada o agente se encontrará em um outro estado e receberá uma recompença pelo feito. O objetivo do agente é maximizar a recompensa total.\n",
        "\n",
        "### Recompensa\n",
        "A recompensa define o objetivo pelo qual o agente deve ser otimizado. O passo de definir quais serão as recompenças deve ser feito com cuidado pois uma definição errônea [pode levar a resultados não desejados](https://blog.openai.com/faulty-reward-functions/). Os problemas e as formas de evita-los fogem do escopo deste tutorial.\n",
        "\n",
        "### Método da entropia cruzada\n",
        "\n",
        "O método da entropia cruzada consiste na utilização do método de Monte Carlo para avaliar a importância de um estimador. O algorítimo por si, consiste na repetição de 2 passos:\n",
        "\n",
        "- Colete amostras da distribuição estimada\n",
        "- Minimize a [entropia cruzada](https://en.wikipedia.org/wiki/Cross_entropy) entre a distribuição desejada e a distribuição estimada para que uma estimativa melhor seja feita no próximo passo.\n",
        "\n",
        "A aprendizagem por reforço busca representar o problema pelo processo de decisão de Markov onde a recompença esperada dada um estado depende apenas dos estados futuros. Para a aprendizagem por reforço nós buscamos modelar o processo de Markov que maximiza a recompença esperada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oVZb7pXdH-M"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Vamos primeiro instalar algumas bibliotecas necessárias para a visualização."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkJu5DfGd0OH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abdec40e-d695-4d87-bbf7-2ff0fc54970e"
      },
      "source": [
        "!apt install -qq xvfb ffmpeg python-opengl -y\n",
        "\n",
        "!pip install -q pyvirtualdisplay\n",
        "!pip install -q piglet\n",
        "!pip install -q gym\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl xvfb\n",
            "0 upgraded, 2 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 1,281 kB of archives.\n",
            "After this operation, 7,686 kB of additional disk space will be used.\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 160772 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.1MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj5rtc95eR5C"
      },
      "source": [
        "Agora podemos importar alguns módulos que vamos usar durante este tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbxHRDIpdHka"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38A1ds9EaIny"
      },
      "source": [
        "## O ambiente\n",
        "Vamos começar primeiramente com o ambiente de simulação e o problema que queremos resolver. Para este caso vamos trabalhar com o controle de um pendulo invertido de forma a manter ele estável. Podemos agradecer a [OpenAi](https://openai.com/) por facilitar o nosso serviço disponibilizando um [ambiente para simulação](https://gym.openai.com/envs/CartPole-v1/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmG-T8Ftbewi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "69d07498-d475-42b8-d150-4ee28c576b4f"
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset()\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "print(\"Espaço de estados:\", env.observation_space)\n",
        "print(\"Possiveis ações:\", env.action_space)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Espaço de estados: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "Possiveis ações: Discrete(2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlOXF-nnZ1-X"
      },
      "source": [
        "## O Modelo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvlktkmchGFt"
      },
      "source": [
        "O modelo deve ter como entrada os estados e como saída as ações, assim, usamos uma rede com 4 neurônios na entrada e 2 neurônios na saída."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvjj2fauP5R7"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "def make_model():\n",
        "  model = Sequential()\n",
        "  model.add(Dense(10, input_dim=4, activation='relu'))\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "  \n",
        "  model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6INeGQjOilWr"
      },
      "source": [
        "model = make_model()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_kuHVQyjDux"
      },
      "source": [
        "## A simulação\n",
        "\n",
        "Agora podemos simular o ambiente para que possamos treinar a nossa rede neural. Primeiro criamos a função que irá realizar a simulação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2fI9Ispjb07"
      },
      "source": [
        "def generate_session(tmax=10000):\n",
        "  states, actions = [], []\n",
        "  total_reward = 0\n",
        "  \n",
        "  state = env.reset()\n",
        "  for t in range(tmax):\n",
        "    probs = model.predict(np.expand_dims(state, axis=0))\n",
        "    action = np.random.choice(2, p=np.ravel(probs))\n",
        "    \n",
        "    new_s, r, done, _ = env.step(action)\n",
        "    \n",
        "    states.append(state)\n",
        "    actions.append(action)\n",
        "    total_reward += r - 0.1*np.abs(state[0])\n",
        "    \n",
        "    state = new_s\n",
        "    if done:\n",
        "      break\n",
        "  return states, actions, total_reward\n",
        "  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWN7vOGImS6h"
      },
      "source": [
        "Queremos então selecionar quais simulações tiveram a melhor recompensa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMBbWOYtmgwY"
      },
      "source": [
        "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
        "  reward_threshold = np.percentile(rewards_batch, percentile)\n",
        "  \n",
        "  elite_states  = np.vstack(states_batch[rewards_batch >= reward_threshold])\n",
        "  elite_actions = np.hstack(actions_batch[rewards_batch >= reward_threshold])\n",
        "  \n",
        "  return elite_states, elite_actions"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-trh0mbnKll"
      },
      "source": [
        "Por último queremos ver o resultado do treinamento, assim, utilizei o código apresentado [aqui](https://github.com/yandexdataschool/Practical_RL), desenvolvido para o curso [Practical Reinforcement Learning](https://www.coursera.org/learn/practical-rl)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOI2lpZcnrOI"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def show_progress(batch_rewards, log, percentile, reward_range=[-990,+10]):\n",
        "    \"\"\"\n",
        "    A convenience function that displays training progress. \n",
        "    No cool math here, just charts.\n",
        "    \"\"\"\n",
        "    \n",
        "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
        "    log.append([mean_reward, threshold])\n",
        "\n",
        "    clear_output(True)\n",
        "    print(\"mean reward = %.3f, threshold=%.3f\"%(mean_reward, threshold))\n",
        "    plt.figure(figsize=[8,4])\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
        "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    \n",
        "    plt.subplot(1,2,2)\n",
        "    plt.hist(batch_rewards, range=reward_range);\n",
        "    plt.vlines([np.percentile(batch_rewards, percentile)], [0], [100], label=\"percentile\", color='red')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCSZCemSoBPM"
      },
      "source": [
        "## Treinamento\n",
        "\n",
        "Podemos agora treinar o nosso modelo e conferir os resultados, para isso selecionaremos os melhores jogos e treinaremos a rede para mapear os estados apresentados para as ações tomadas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Moj3TZKoWqn"
      },
      "source": [
        "from tesorflow.keras.utils import to_categorical\n",
        "\n",
        "n_sessions = 50\n",
        "percentile = 75\n",
        "log = []\n",
        "\n",
        "for i in range(100):\n",
        "    #generate new sessions\n",
        "    sessions = [generate_session(tmax=int(1e7)) for _ in tqdm.trange(n_sessions)]\n",
        "    \n",
        "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
        "    \n",
        "    elite_states, elite_actions = select_elites(batch_states,batch_actions,batch_rewards, percentile=percentile)\n",
        "    model.fit(elite_states, to_categorical(elite_actions))\n",
        "    show_progress(batch_rewards, log, percentile, reward_range=[np.min(batch_rewards), np.max(batch_rewards)])\n",
        "    \n",
        "    model.save('./inverted_pendulum.h5')\n",
        "    \n",
        "    if np.mean(batch_rewards) > 499:\n",
        "      print(\"Training complete\")\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwPJDCDnCOwR"
      },
      "source": [
        "## Resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMgN8sbsCOI4"
      },
      "source": [
        "#record sessions\n",
        "import gym.wrappers\n",
        "env = gym.wrappers.Monitor(gym.make(\"CartPole-v1\"), directory=\"videos\", force=True)\n",
        "sessions = [generate_session(int(1e5)) for _ in range(100)]\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krl7iq9rjUQX"
      },
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "video_names = list(filter(lambda s:s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
        "files.download('./videos/'+video_names[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYrjO652bgYk"
      },
      "source": [
        "!ls videos"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}